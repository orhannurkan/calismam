{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "\n",
    "Everyone has already heard about the gradient descent algorithm, but... do you really know how it works and have you already implemented it by yourself to make sure you understand how it works?\n",
    "\n",
    "Using modules that do the calculations for us is cool.\n",
    "\n",
    "Understanding what you're manipulating is better!\n",
    "\n",
    "This is what we will do in this article, in three steps:\n",
    "\n",
    "1. What is gradient descent?\n",
    "2. How does it work?\n",
    "3. And what are the pitfalls to avoid?\n",
    "\n",
    "The only prerequisites for this article are to know what a derivative is.\n",
    "\n",
    "Let's go! \n",
    "\n",
    "## What is gradient descent? \n",
    "\n",
    "It is an algorithm to find the minimum of a function.\n",
    "\n",
    "It is a problem that is found everywhere in mathematics.\n",
    "\n",
    "And this is also the case in data science, especially when you want to minimize the error rate that is rectified during backpropagation.\n",
    "\n",
    "**The gradient descent offers an approach:**\n",
    "\n",
    "- algorithmic\n",
    "- iterative\n",
    "- which works pretty well in most cases\n",
    "\n",
    "Well sometimes we can have [vanish gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), but we will investigate this later. \n",
    "\n",
    "### How does it work?  \n",
    "Good imagine that you are at the top of a mountain without a guide, without a map and that you want to go down to the lowest point of the mountain. (i.e. where the altitude is lowest)\n",
    "\n",
    "Your approach will be to face the slope, and you go in that direction for a few minutes.\n",
    "\n",
    "![mountain](./img/montagne.png)\n",
    "\n",
    "A few minutes later, you are at a new point.\n",
    "\n",
    "Again, you face the slope and move in that direction for a few minutes.\n",
    "\n",
    "And so on...\n",
    "\n",
    "And after a while, always going down, you will go to the lowest point of altitude.\n",
    "\n",
    "Easy!\n",
    "\n",
    "### How does it work mathematically?\n",
    "\n",
    "In math, the slope is the derivative:\n",
    "![](./img/derivate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The value of the derivative corresponds to the inclination of the slope at a given point.\n",
    "\n",
    "So...\n",
    "\n",
    "- If the derivative is high, it means that the slope is very steep.\n",
    "- And if the derivative is low, the slope is low.\n",
    "- If the derivative is 0, it's flat. \n",
    "- And if the derivative is negative, it means that it goes down (when you go to the right!).\n",
    "\n",
    "So once you have the value of the slope, how do you go down?\n",
    "\n",
    "We're going the other way around the slope!\n",
    "\n",
    "Positive derivative => slope going up to the right => we go to the left.\n",
    "Negative derivative => slope going down to the right => we go to the right.\n",
    "\n",
    "But by how much?\n",
    "\n",
    "Do we take a single step, two steps, continue for how long?\n",
    "\n",
    "In fact, we would like to take just one step, restate the question of the derivative, then take another step, etc.\n",
    "\n",
    "Except that it's going to be very computationally intensive (we're going to make a lot of decisions) if we do that. And so it's going to be slow.\n",
    "\n",
    "But on the other hand, if you take big steps, you risk missing the minimum, so come back in the other direction, exceed the minimum again, and so on, without ever falling on it.\n",
    "\n",
    "You just have to find the right balance!\n",
    "\n",
    "This is done by specifying what is called a learning rate. We'll talk about it a little later.\n",
    "\n",
    "### More concretely ...\n",
    "\n",
    "Let's try to find the lowest point of this curve. The objective is to find the minimum that we see on the right, around x between 3 and 4.  \n",
    "\n",
    "![](./img/function.png)\n",
    "\n",
    "In this case, we could calculate the derivative equal to 0, but the goal is to understand the descent of the gradient, so that's what we're going to do.\n",
    "\n",
    "Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return 2 * x * x * np.cos(x) - 5 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's study it on the interval[-5,5]:\n",
    "\n",
    "There are 3 steps:\n",
    "\n",
    "1. We take a point at random x0.\n",
    "2. The value of the slope is calculated at f(x0).\n",
    "3. We move in the opposite direction to the slope. \n",
    "\n",
    "#### First step :\n",
    "We take a point at random, here x0=-1. This corresponds to f(x0)=6.08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0806046117362795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [-1.]\n",
    "f(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the curve, it represents the following \n",
    "![](./img/function-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "Calculate the value of the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.478267253856766"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df(x):\n",
    "    return 4 * x * np.cos(x) - 2 * x * x * np.sin(x) - 5\n",
    "\n",
    "slope = df(x[0])\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore have a negative slope equal to  ``-5.47``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 \n",
    " We move in the opposite direction to the slope:  \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>0</mn>\n",
    "  </msub>\n",
    "  <mo>&#x2212;<!-- − --></mo>\n",
    "  <mi>&#x03B1;<!-- α --></mi>\n",
    "  <mo>&#x2217;<!-- ∗ --></mo>\n",
    "  <msup>\n",
    "    <mi>f</mi>\n",
    "    <mo>&#x2032;</mo>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>0</mn>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "How to choose the value of ``α`` ?\n",
    "\n",
    "This is the learning rate. I propose for the moment to test a small value α=0.05, and we will test other values a little later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7260866373071617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "x.append(x[0] - alpha * slope)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our new value for our point! Let's display it:\n",
    "![](./img/function-3.png)\n",
    "\n",
    "\n",
    "We moved a little bit. This approach is iterative, which means that the operation will have to be repeated several times to achieve the minimum.\n",
    "\n",
    "Let's start over:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4024997370140509"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.append(x[1] - alpha * df(x[1]))\n",
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/function-4.png)\n",
    "\n",
    "We're moving slowly.\n",
    "\n",
    "After a little over a dozen iterations, our algorithm converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1.]\n",
    "for i in range(20):\n",
    "    x.append(x[i] - alpha * df(x[i]))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/gradientdescent-alpha0.05.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our little ball ends up reaching the minimum and staying there, at about x=3.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pitfalls to avoid\n",
    "\n",
    "There, we saw the principle of gradient descent.\n",
    "\n",
    "On a simple example.\n",
    "\n",
    "But in reality, it often happens that we encounter problems. For example:\n",
    "\n",
    "- How to set the learning rate?\n",
    "- How to solve the vanishing gradient problem?\n",
    "- How to fight against local minima?\n",
    "\n",
    "\n",
    "How to set the learning rate?\n",
    "In the previous example, we set ourselves α=0.05.\n",
    "\n",
    "Why? \n",
    "\n",
    "I cheated a little.\n",
    "\n",
    "I tested it before and I saw that it was a value that worked well.\n",
    "\n",
    "In fact, it is necessary to find the right balance by taking into account that:\n",
    "\n",
    "The higher the value α is, the faster we will move forward, but the algorithm may never converge.\n",
    "The smaller the value α is, the slower we will move forward, and so the longer it will take to converge.  \n",
    "\n",
    "For example, with a value α=0.2, we obtain :\n",
    "\n",
    "![](./img/gradientdescent-alpha0.2.gif)\n",
    "\n",
    "Now we see that we have a convergence problem. The value α is too high.\n",
    "\n",
    "As a result, when the skier faces the slope, he advances so much that he finds himself on the other side of the mountain.\n",
    "\n",
    "If we take a value too small, like α=0.001 \n",
    "\n",
    "![](./img/gradientdescent-alpha0.001.gif)\n",
    "\n",
    "It works, it converges, except it takes a lot longer! (The animation is accelerated 5 times faster)\n",
    "\n",
    "For our simple problem, the impact is small, but when you train neural networks, it makes a big difference in computing time!\n",
    "\n",
    "Unfortunately, there is no magic formula for finding the perfect learning rate.\n",
    "\n",
    "To find it, you have to test several of them and get the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to solve the problem of the vanishing gradient?\n",
    "\n",
    "There are two very common problems in Deep Learning, exploding gradient and vanishing gradient. In the first case, it means having a too high learning rate that will cause instability of the algorithm. In Deep Learning, this can happen when you have a very large network. As the gradients of each layer are multiplied between them, we can very quickly have a gradient that explodes exponentially. For the vanishing gradient, it's the opposite.\n",
    "\n",
    "The gradient becomes so small that our skier hardly makes any progress. This can happen if the learning rate is too low, as we have seen. But it can also happen if our skier is stuck on a kind of plateau.\n",
    "\n",
    "Imagine the following function:\n",
    "How to solve the problem of the vanishing gradient?\n",
    "\n",
    "There are two very common problems in Deep Learning, exploding gradient and vanishing gradient.\n",
    "\n",
    "In the first case, it means having a too high learning rate that will cause instability of the algorithm.\n",
    "\n",
    "In Deep Learning, this can happen when you have a very large network. As the gradients of each layer are multiplied between them, we can very quickly have a gradient that explodes exponentially.\n",
    "\n",
    "For the vanishing gradient, it's the opposite.\n",
    "\n",
    "The gradient becomes so small that our skier hardly makes any progress. This can happen if the learning rate is too low, as we have seen.\n",
    "\n",
    "But it can also happen if our skier is stuck on a kind of tray.\n",
    "\n",
    "Imagine the following function:  \n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>arctan</mi>\n",
    "  <mo>&#x2061;</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msup>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "![plateau](./img/plate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the minimum is in x=0.\n",
    "\n",
    "Suppose that the random point falls on a value far from 0, such as -20.\n",
    "\n",
    "I choose a very high learning rate compared to the previous examples, at α=1, and I get a very long algorithm to converge:\n",
    "\n",
    "![vanish](./img/vanish.gif) \n",
    "\n",
    "But imagine a more complex function that is a mixture of this one, with a long flat plateau, and roller coasters in other places. At that point, whatever the learning rate you choose, you will have problems. Either a very slow convergence or an instability of the algorithm.\n",
    "\n",
    "In the Deep Learning, this type of problem is solved with the ReLU functions. We'll see about that later.\n",
    "\n",
    "### How to fight against local minima?\n",
    "Let us consider this time the following function:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>x</mi>\n",
    "  <mi>cos</mi>\n",
    "  <mo>&#x2061;<!-- ⁡ --></mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "![](./img/loc.png)\n",
    "\n",
    "The problem with this function is that there are many local minima, i. e. troughs, which are not the global minimum (which, over this interval, is on the right, around x=9.5).\n",
    "\n",
    "The following animation shows some examples where the starting point varies:\n",
    "![](./img/var.gif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the convergence point depends a lot on the initial point.\n",
    "\n",
    "Sometimes he will be able to find the global minimum, and other times, the algorithm will get stuck in a local minimum.\n",
    "\n",
    "One technique to avoid this problem is to run the algorithm several times,  \n",
    "and to keep the smallest of the minima, but obviously it is more intensive for the cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
