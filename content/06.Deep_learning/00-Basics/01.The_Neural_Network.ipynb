{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the neural network ?\n",
    "A neural network is a set of interconnected formal neurons that solves complex problems such as pattern recognition or natural language processing by adjusting weighting coefficients in a learning phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Operation of the neural network\n",
    "A neural network is inspired by the functioning of biological neurons and takes shape in a computer as an algorithm.  \n",
    "The neural network can modify itself according to the results of its actions, which allows learning and problem solving without algorithms, and therefore without traditional programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![biologic vs artificial neuron](./img/bioVSart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we can model this neuron according to a set of inputs, which receives a weighted sum of these inputs using weights, adds bias and provides a number based on an activation function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one neuron](./img/nl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The weights** can be considered as a set of buttons that can be adjusted to obtain different outputs.  \n",
    "\n",
    "\n",
    "- **Bias** can be considered as another button that decides when a neuron remains inactive or, in other words, to what extent the weighted sum must be high for the neuron to be significantly active.  \n",
    "\n",
    "\n",
    "- **The activation function** We will see in more detail the next paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an activation function?  \n",
    "\n",
    "The activation function was inspired by the \"action potential\", an electrical phenomenon between two biological neurons.\n",
    "\n",
    "Let's start with a quick reminder of the biology class: A neuron has a cell body, an axon that allows it to send messages to other neurons and dendrites that allow it to receive signals from other neurons.\n",
    "\n",
    "![Image de neuronne](./img/cn.jpg)\n",
    "\n",
    "The neuron receives signals from other neurons through the dendrites. The weight associated with a dendrite, called synaptic weight, is multiplied by the incoming signal. Dendrite signals are accumulated in the cell body and if the resulting signal strength exceeds a certain threshold, the neuron transmits the message to the axon. Otherwise, the signal is killed by the neuron and does not spread any further. The action potential is therefore the variation in signal strength indicating whether or not communication should take place.\n",
    "\n",
    "The activation function decides whether or not to transmit the signal. In this case, it is a simple function with only one parameter: the threshold. Now, when we learn something new, the threshold and connection probability (called synaptic weight) of some neurons change. This creates new connections between neurons, allowing the brain to learn new things.\n",
    "\n",
    "Now let's see how all this works with an artificial neural network: The incoming values in a neuron (x1, x2, x3, ..., xn) are multiplied with their associated weights (reference to synaptic weight) (w1, w2, w3, ..., wn). These multipications are then summed and the bias (reference to the threshold) is added. The image below shows the calculation formula.\n",
    "\n",
    "The purpose of activation is to transform the signal so as to obtain an output value from complex transformations between the inputs. To do this, the activation function must be non-linear. It is this non-linearity that makes it possible to create such transformations.\n",
    "\n",
    "The most commonly used examples of activation functions are sigmoid, softmax, ReLU, tanh, etc.\n",
    "\n",
    "![factivation functions](./img/reLU.png)\n",
    "\n",
    "**Sigmoid :**  \n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "**Tanh or hyperbolic tangent :**  \n",
    "Tanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.\n",
    "\n",
    "**ReLU (Rectified Linear Unit) :**  \n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "There are others that are less used. I'll let you check Google to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the network\n",
    "Now that we know how a single neuron works, we can connect them together to form a network in the form of layers.  \n",
    "So an artificial neural network is just an overrated composite function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![multicouche](./img/multi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical neural network consists of 3 types of layers:  \n",
    "\n",
    "- **The input layer:** The given data points are fed into this layer. There can be only 1 input layer. The number of neurons in this layer is equal to the number of inputs. Let's imagine that in input there is a photo that is 28 pixels by 28 pixels. So we need 1 neuron per pixel, which makes 784 Neurons    \n",
    "\n",
    "\n",
    "- **The hidden layers:** These are the layers that try to find patterns in the inputs to get the outputs we need. A network can have an unlimited number of hidden layers.  \n",
    "\n",
    "\n",
    "- **The output layer:** This layer gives us the predictions of the network, ie. the outputs that the network thinks should be correct given its current parameters (weights and biases each neuron). The number of neurons in this layer is equal to the number of values we need to predict. Imagine that we want to create an algorithm that recognizes numbers. Since there are ten digits to recognize (0 to 9), there will be 10 neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural network](./img/3lnn.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
