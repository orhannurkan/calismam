{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "In the previous chapter, we saw how to create a simple network. These were simple images with little information, in black and white. The patterns were repeated quite often and the shapes were quite simple. But this kind of architecture would not work with more complex images, such as that of a cat for example. The computation time would be so long that it would become unusable. \n",
    "\n",
    "To solve this type of problem, it is preferable to use a so-called convolution architecture. This is what we will see in this chapter. \n",
    "\n",
    "\n",
    "\n",
    "## What is a convolutional ?\n",
    "\n",
    "A convolution is an operation that changes a function into something else. We do convolutions so that we can transform the original function into a form to get more information.\n",
    "\n",
    "Convolutions have been used for a long time in image processing to blur and sharpen images, and perform other operations, such as, enhance edges and emboss.\n",
    "\n",
    "![Conv](./img/conv.jpg)\n",
    "\n",
    "Here, the original image is the one on the left and the matrix of numbers in the middle is the convolutional matrix or filter.\n",
    "\n",
    "A convolution operation is an element wise matrix multiplication operation. Where one of the matrices is the image, and the other is the filter or kernel that turns the image into something else. The output of this is the final convoluted image.\n",
    "\n",
    "![schema](./img/schema.gif)\n",
    "\n",
    "f the image is larger than the size of the filter, we slide the filter to the various parts of the image and perform the convolution operation. Each time we do that, we generate a new pixel in the output image.\n",
    "\n",
    "The number of pixels by which we slide the kernel is known as the stride. The stride is usually kept as 1, but we can increase it. When increased, we might have to increase the size of the image by a few pixels to fit in the kernel at the edges of the image. This increase is called padding.\n",
    "\n",
    "I’ll talk more about how this can help us get more information from an image in a later section.\n",
    "\n",
    "### CONVOLUTIONAL FILTERS IN MACHINE LEARNING\n",
    "\n",
    "Convolutions aren’t a new concept. They have been used in image and signal processing for a long time. However, convolutions in machine learning are different than those in image processing.\n",
    "\n",
    "In image processing, there are a set few filters that are used to perform a few tasks. For example, a filter that can be used to blur images may look like this:\n",
    "\n",
    "![filter 2](./img/filter2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas, a filter that does the opposite, sharpen an image, looks like this:\n",
    "\n",
    "![filter2](./img/04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other filters, like sobel filters, can perform an edge detection and other operations.\n",
    "![filter 4](./img/05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNNs, filters are not defined. The value of each filter is learned during the training process.\n",
    "\n",
    "By being able to learn the values of different filters, CNNs can find more meaning from images that humans and human designed filters might not be able to find.\n",
    "\n",
    "More often than not, we see the filters in a convolutional layer learn to detect abstract concepts, like the boundary of a face or the shoulders of a person. By stacking layers of convolutions on top of each other, we can get more abstract and in-depth information from a CNN.\n",
    "\n",
    "A second layer of convolution might be able to detect the shapes of eyes or the edges of a shoulder and so on. This also allows CNNs to perform hierarchical feature learning; which is how our brains are thought to identify objects.\n",
    "\n",
    "![image 5](./img/06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image, we can see how the different filters in each CNN layer interprets the number 0.\n",
    "\n",
    "It is this ability of CNNs to be able to detect abstract and complex features that makes them so attractive in image recognition problems.\n",
    "\n",
    "Depending on the kind of problem we are solving and the types of features we are trying to learn, we use different kinds of convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 2D convolution layer \n",
    "The most common type of convolution that is used is the 2D convolution layer, and is usually abbreviated as conv2D. A filter or a kernel in a conv2D layer has a height and a width. They are generally smaller than the input image and so we move them across the whole image. The area where the filter is on the image is called the receptive field.\n",
    "\n",
    "Working: Conv2D filters extend through the three channels in an image (Red, Green, and Blue). The filters may be different for each channel too. After the convolutions are performed individually for each channels, they are added up to get the final convoluted image. The output of a filter after a convolution operation is called a feature map.\n",
    "\n",
    "![image 7](./img/07.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each filter in this layer is randomly initialized to some distribution (Normal, Gaussian, etc.). By having different initialization criteria, each filter gets trained slightly differently. They eventually learn to detect different features in the image.\n",
    "\n",
    "If they were all initialized similarly, then the chances of two filters learning similar features increase dramatically. Random initialization ensures that each filter learns to identify different features.\n",
    "\n",
    "Since each conv2D filter learns a separate feature, we use many of them in a single layer to identify different features. The best part is that every filter is learnt automatically.\n",
    "\n",
    "Each of these filters are used as inputs to the next layer in the neural network.\n",
    "\n",
    "If there are 8 filters in the first layer and 32 in the second, then each filter in the second layer sees 8 filter inputs. Meaning that we get 32X8 feature maps in the second layer. Each of the 8 feature maps of a single filter are added to get a single output from each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What the conv2D layer is doing:**  \n",
    "\n",
    "Each filter in the conv2D layer is a matrix of numbers. The matrix corresponds to a pattern or feature that the filter is looking for.\n",
    "\n",
    "In the image below, the filter is looking for a curved line. That curved line could correspond to the back of a mouse, or a part of the numbers 8, 9, 0, etc. Whenever the filter comes across a pattern like that in the image, it gives a high output.\n",
    "\n",
    "\n",
    "![image 8](./img/08.jpg)\n",
    "\n",
    "Although this may seem like a very simple example, most conv2D filters in the first layer of a CNN search for similar features. It also means that the same filter can be used to extract information from multiple types of images (mouse, numbers, faces and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where the conv2D layer is used:**  \n",
    "\n",
    "These are used in the first few convolutional layers of a CNN to extract simple features. They have also been used in capsule networks. Previously, they were the sole filters used and they made up most of a CNN. For example, the original LeNet architecture and the AlexNet architecture mostly used conv2D filters.\n",
    "\n",
    "Nowadays, with advancements in convolutional layers and filters, more sophisticated filters have been designed that can serve different purposes and can be used for different applications. We’ll look at some of them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use them while designing a CNN:**  \n",
    "\n",
    "\n",
    "Conv2D filters are used only in the initial layers of a Convolutional Neural Network. They are put there to extract the initial high level features from an image.\n",
    "\n",
    "While there are many rules of thumb for designing such filters, they are generally stacked with an increasing number of filters in each layer. Each successive layer can have two to four times the number of filters in the previous layer. This helps the network learn hierarchical features.\n",
    "\n",
    "**Limitations of the conv2D layer:**   \n",
    "\n",
    "The conv2D layer works fairly impressively. However, they do have certain limitations, which prompted researchers to find alternatives to the conv2D layer.\n",
    "\n",
    "Their biggest limitation is the fact that they are very computationally expensive. A large conv2D filter will take a lot of time to compute and stacking many of them in layers will increase the amount of computations.\n",
    "\n",
    "An easy solution to this is to decrease the size of the filters and increase the strides. While you can do that, it also reduces the effective receptive field of the filter and reduces the amount of information that it can capture. In fact, in the first Convolutional Network paper, Yann Le Cunn, had mentioned his fear of having a 1x1 convolutional filter.\n",
    "\n",
    "However, before we look at the other types of convolutions, it is best to get a more intuitive understanding of filters.\n",
    "\n",
    "Conv2D layers are generally used for achieving high accuracy in image recognition tasks. However, they require a lot of calculations to be done and are very RAM intensive.\n",
    "\n",
    "Dilated or Atrous Convolutions reduces the complexity of the convolution operation. This means that they can be used in real time applications and in applications where the processing power is less like in smartphones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polling layer**\n",
    "\n",
    "Note: Pooling is not a convolutional layer, but we are talking about it here, as it is a layer that is used commonly in CNNs.\n",
    "\n",
    "The pooling layer was introduced for two main reasons: The first was to perform downsampling, that is, to reduce the amount of computation that needs to be done, and the second to send only the important data to the next layers in the CNNs.\n",
    "\n",
    "How they work: There are two kinds of pooling layers: max pooling and average pooling.\n",
    "\n",
    "In max pooling, we take only the value of the largest pixel among all the pixels in the receptive field of the filter. In the case of average pooling, we take the average of all the values in the receptive field.\n",
    "\n",
    "![image11](./img/11.jpg)\n",
    "\n",
    "There are many arguments about which one is better and there are many rules of thumb about when to use which, but, typically max pooling is more commonly used.\n",
    "\n",
    "Since we try to downsample the input vector, pooling kernels do not overlap, i.e., they have a stride that is larger than the size of the kernel itself.\n",
    "\n",
    "**Limitations of the Pooling layer:** \n",
    "\n",
    "Downsampling by pooling causes lots of problems in CNNs.\n",
    "\n",
    "The pooling layer loses positional information about the different objects inside the image. This is why, many new architectures have stopped using the pooling layer altogether.\n",
    "\n",
    "The pooling layer was introduced to reduce computational time and complexity by reducing the number of parameters. With the rise in computational power and the presence of better ways of downsampling, like Separable and Dilated Convolutions, the pooling layer can be cast aside.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us look at this in a more concrete way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to study the parameters Keras Conv2D class, and we will see the most important parameters you need to tune when training your own Convolutional Neural Networks (CNNs). From there we are going to use the Keras Conv2D class to implement a simple CNN. We’ll then train and evaluate this CNN on the CIFAR-10 dataset.\n",
    "\n",
    "**Goals**\n",
    "\n",
    "1. Quickly determine if you need to utilize a specific parameter to the Keras Conv2D class\n",
    "2. Decide on a proper value for that specific parameter\n",
    "3. Effectively train your own Convolutional Neural Network\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let’s go ahead and get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras Conv2D class\n",
    "The Keras Conv2D class constructor has the following signature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "model.add(Conv2D(filters, kernel_size, strides=(1, 1),\n",
    "  padding='valid', data_format=None, dilation_rate=(1, 1),\n",
    "  activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "  bias_initializer='zeros', kernel_regularizer=None,\n",
    "  bias_regularizer=None, activity_regularizer=None,\n",
    "  kernel_constraint=None, bias_constraint=None))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit overwhelming, right?\n",
    "\n",
    "How in the world are you supposed to properly set these values?\n",
    "\n",
    "No worries — let’s examine each of these parameters individually, giving you a strong understanding of not only what each parameter controls but also how to properly set each parameter as well.\n",
    "\n",
    "### Filters\n",
    "\n",
    "![image filter](./img/filters.png)\n",
    "> **Figure 1:** The Keras Conv2D parameter, filters determines the number of kernels to convolve with the input volume. Each of these operations produces a 2D activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first required Conv2D parameter is the number of filters  that the convolutional layer will learn.\n",
    "\n",
    "Layers early in the network architecture (i.e., closer to the actual input image) learn fewer convolutional filters while layers deeper in the network (i.e., closer to the output predictions) will learn more filters.\n",
    "\n",
    "Conv2D layers in between will learn more filters than the early Conv2D layers but fewer filters than the layers closer to the output. Let’s go ahead and take a look at an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python \n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Line 1 we learn a total of 32  filters. Max pooling is then used to reduce the spatial dimensions of the output volume.\n",
    "\n",
    "We then learn 64  filters on Line 4. Again max pooling is used to reduce the spatial dimensions.\n",
    "\n",
    "The final Conv2D layer learns 128  filters.\n",
    "\n",
    "Notice at as our output spatial volume is decreasing our number of filters learned is increasing — this is a common practice in designing CNN architectures and one I recommend you do as well. As far as choosing the appropriate number of filters , I nearly always recommend using powers of 2 as the values.\n",
    "\n",
    "You may need to tune the exact value depending on (1) the complexity of your dataset and (2) the depth of your neural network, but I recommend starting with filters in the range [32, 64, 128] in the earlier and increasing up to [256, 512, 1024] in the deeper layers.\n",
    "\n",
    "Again, the exact range of the values may be different for you, but start with a smaller number of filters and only increase when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel size\n",
    "![image 12](./img/12.png)\n",
    "> **Figure 2:** The Keras deep learning Conv2D parameter, filter_size, determines the dimensions of the kernel. Common dimensions include 1×1, 3×3, 5×5, and 7×7 which can be passed as (1, 1), (3, 3), (5, 5), or (7, 7) tuples.\n",
    "\n",
    "The second required parameter you need to provide to the Keras Conv2D class is the kernel_size , a 2-tuple specifying the width and height of the 2D convolution window.\n",
    "\n",
    "The kernel_size  must be an odd integer as well.\n",
    "\n",
    "Typical values for kernel_size  include: (1, 1) , (3, 3) , (5, 5) , (7, 7) . It’s rare to see kernel sizes larger than 7×7.\n",
    "\n",
    "So, when do you use each?\n",
    "\n",
    "If your input images are greater than 128×128 you may choose to use a kernel size > 3 to help (1) learn larger spatial filters and (2) to help reduce volume size.\n",
    "\n",
    "Other networks, such as VGGNet, exclusively use (3, 3)  filters throughout the entire network.\n",
    "\n",
    "More advanced architectures such as Inception, ResNet, and SqueezeNet design entire micro-architectures which are “modules” inside the network that learn local features at different scales (i.e., 1×1, 3×3, and 5×5) and then combine the outputs.\n",
    "\n",
    "A great example can be seen in the Inception module below:\n",
    "\n",
    "![image13](./img/13.png)\n",
    "> **Figure 3:** The Inception/GoogLeNet CNN architecture uses “micro-architecture” modules inside the network that learn local features at different scales (filter_size) and then combine the outputs.\n",
    "\n",
    "\n",
    "The Residual module in the ResNet architecture uses 1×1 and 3×3 filters as a form of dimensionality reduction which helps to keep the number of parameters in the network low (or as low as possible given the depth of the network):\n",
    "\n",
    "![image13](./img/14.png)\n",
    "> **Figure 4:** The ResNet “Residual module” uses 1×1 and 3×3 filters for dimensionality reduction. This helps keep the overall network smaller with fewer parameters.\n",
    "\n",
    "**So, how should you choose your filter_size ?**\n",
    "\n",
    "First, examine your input image — is it larger than 128×128?\n",
    "\n",
    "If so, consider using a 5×5 or 7×7 kernel to learn larger features and then quickly reduce spatial dimensions — then start working with 3×3 kernels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "model.add(Conv2D(32, (7, 7), activation=\"relu\"))\n",
    "...\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "\n",
    "````\n",
    "\n",
    "If your images are smaller than 128×128 you may want to consider sticking with strictly 1×1 and 3×3 filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides\n",
    "\n",
    "The strides  parameter is a 2-tuple of integers, specifying the “step” of the convolution along the x and y axis of the input volume.\n",
    "\n",
    "The strides  value defaults to (1, 1) , implying that:\n",
    "\n",
    "A given convolutional filter is applied to the current location of the input volume\n",
    "The filter takes a 1-pixel step to the right and again the filter is applied to the input volume\n",
    "This process is performed until we reach the far-right border of the volume in which we move our filter one pixel down and then start again from the far left\n",
    "Typically you’ll leave the strides  parameter with the default (1, 1)  value; however, you may occasionally increase it to (2, 2)  to help reduce the size of the output volume (since the step size of the filter is larger).\n",
    "\n",
    "Typically you’ll see strides of 2×2 as a replacement to max pooling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), activation=\"relu\"))\n",
    "model.add(Conv2D(128, (3, 3), strides=(2, 2), activation=\"relu\"))\n",
    "```\n",
    "\n",
    "Here we can see our first two Conv2D layers have a stride of 1×1. The final Conv2D layer; however, takes the place of a max pooling layer, and instead reduces the spatial dimensions of the output volume via strided convolution.\n",
    "\n",
    "In 2014, Springenber et al. published a paper entitled Striving for Simplicity: [The All Convolutional](https://arxiv.org/abs/1412.6806) Net which demonstrated that replacing pooling layers with strided convolutions can increase accuracy in some situations.\n",
    "\n",
    "ResNet, a popular CNN, has embraced this finding — if you ever look at the source code to a ResNet implementation (or implement it yourself), you’ll see that ResNet replies on strided convolution rather than max pooling to reduce spatial dimensions in between residual modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "![image15](./img/15.gif)\n",
    "> **Figure 5:** A 3×3 kernel applied to an image with padding. The Keras Conv2D padding parameter accepts either \"valid\" (no padding) or \"same\" (padding + preserving spatial dimensions). This animation was contributed to StackOverflow [(source)](https://stackoverflow.com/questions/52067833/how-to-plot-an-animated-matrix-in-matplotlib).\n",
    "\n",
    "The padding  parameter to the Keras Conv2D class can take on one of two values: valid  or same .\n",
    "\n",
    "With the valid  parameter the input volume is not zero-padded and the spatial dimensions are allowed to reduce via the natural application of convolution.\n",
    "\n",
    "The following example would naturally reduce the spatial dimensions of our volume:\n",
    "\n",
    "```Python\n",
    "model.add(Conv2D(32, (3, 3), padding=\"valid\"))\n",
    "```\n",
    "\n",
    "If you instead want to preserve the spatial dimensions of the volume such that the output volume size matches the input volume size, then you would want to supply a value of same  for the padding :\n",
    "\n",
    "```Python\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "```\n",
    "\n",
    "While the default Keras Conv2D value is valid  I will typically set it to same  for the majority of the layers in my network and then either reduce spatial dimensions of my volume by either:  \n",
    "\n",
    "1. Max pooling\n",
    "2. Strided convolution\n",
    "\n",
    "I would recommend that you use a similar approach to padding with the Keras Conv2D class as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_format\n",
    "\n",
    "![image 16](./img/16.png)\n",
    "> **Figure 6:** Keras, as a high-level framework, supports multiple deep learning backends. Thus, it includes support for both “channels last” and “channels first” channel ordering.\n",
    "\n",
    "\n",
    "\n",
    "The data format value in the Conv2D class can be either channels_last  or channels_first :\n",
    "\n",
    "- The TensorFlow backend to Keras uses channels last ordering.\n",
    "- The Theano backend uses channels first ordering.\n",
    "\n",
    "You typically shouldn’t have to ever touch this value as Keras for two reasons:\n",
    "\n",
    "1. You are more than likely using the TensorFlow backend to Keras\n",
    "2. And if not, you’ve likely already updated your ~/.keras/keras.json  configuration file to set your backend and associated channel ordering\n",
    "\n",
    "My advice is to never explicitly set the data_format  in your Conv2D class unless you have a very good reason to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dilation_rate \n",
    "\n",
    "![image 17](./img/17.png)\n",
    "> **Figure 7:** The Keras deep learning Conv2D parameter, dilation_rate, accepts a 2-tuple of integers to control dilated convolution [(source)](http://www.erogol.com/dilated-convolution/).\n",
    "\n",
    "The dilation_rate  parameter of the Conv2D class is a 2-tuple of integers, controlling the dilation rate for dilated convolution. Dilated convolution is a basic convolution only applied to the input volume with defined gaps, as Figure 7 above demonstrates.\n",
    "\n",
    "You may use dilated convolution when:\n",
    "\n",
    "You are working with higher resolution images but fine-grained details are still important\n",
    "You are constructing a network with fewer parameters\n",
    "Discussing dilated convolution is outside the scope of this tutorial so if you are interested in learning more, please [refer to this tutorial](http://www.erogol.com/dilated-convolution/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation \n",
    "![activation](./img/18.png)\n",
    "> **Figure 8:** Keras provides a number of common activation functions. The activation parameter to Conv2D is a matter of convenience and allows the activation function for use after convolution to be specified.\n",
    "\n",
    "\n",
    "\n",
    "The activation  parameter to the Conv2D class is simply a convenience parameter, allowing you to supply a string specifying the name of the activation function you want to apply after performing the convolution.\n",
    "\n",
    "In the following example we perform convolution and then apply a ReLU activation function:  \n",
    "\n",
    "````python\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "\n",
    "````\n",
    "\n",
    "The above code is equivalent to:\n",
    "\n",
    "````python\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "````\n",
    "Use the activation  parameter if you and if it helps keep your code cleaner — it’s entirely up to you and won’t have an impact on the performance of your Convolutional Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use_bias\n",
    "The use_bias  parameter of the Conv2D class controls whether a bias vector is added to the convolutional layer.\n",
    "\n",
    "Typically you’ll want to leave this value as True , although some implementations of ResNet will leave the bias parameter out.\n",
    "\n",
    "I recommend keep the bias unless you have a good reason not to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel_initializer and bias_initializer\n",
    "![19](./img/19.png)\n",
    "\n",
    "> **Figure 9:** Keras offers a number of initializers for the Conv2D class. Initializers can be used to help train deeper neural networks more effectively.\n",
    "\n",
    "The kernel_initializer  controls the initialization method used to initialize all values in the Conv2D class prior to actually training the network.\n",
    "\n",
    "Similarly, the bias_initializer  controls how the bias vector is initialized before training starts.\n",
    "\n",
    "A full list of initializers can be found in the Keras documentation; however, here is what I recommend:\n",
    "\n",
    "1. Leave the bias_initialization  alone — it will by default filled with zeros (you’ll rarely if ever, have to change the bias initialization method.  \n",
    "\n",
    "\n",
    "2. The kernel_initializer  defaults to glorot_uniform , the Xavier Glorot uniform initialization method, which is perfectly fine for the majority of tasks; however, for deeper neural networks you may want to use  he_normal  (MSRA/He et al. initialization) which works especially well when your network has a large number of parameters (i.e., VGGNet).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel_regularizer, bias_regularizer, and activity_regularizer\n",
    "![20](./img/20.png)\n",
    "\n",
    "> **Figure 10:** Regularization hyperparameters should be adjusted especially when working with large datasets and really deep networks. The kernel_regularizer parameter in particular is one that I adjust often to reduce overfitting and increase the ability for a model to generalize to unfamiliar images.\n",
    "\n",
    "The kernel_regularizer , bias_regularizer , and activity_regularizer  control the type and amount of regularization method applied to the Conv2D layer.\n",
    "\n",
    "Applying regularization helps you to:\n",
    "\n",
    "1. Reduce the effects of overfitting\n",
    "2. Increase the ability of your model to generalize\n",
    "\n",
    "**When working with large datasets and deep neural networks applying regularization is typically a must.**\n",
    "\n",
    "Normally you’ll encounter either L1 or L2 regularization being applied — I will use L2 regularization on my networks if I detect signs of overfitting:\n",
    "\n",
    "```python\n",
    "from keras.regularizers import l2\n",
    "...\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "\tkernel_regularizer=l2(0.0005))\n",
    "```\n",
    "\n",
    "\n",
    "The amount of regularization you apply is a hyperparameter you will need to tune for your own dataset, but I find values of 0.0001-0.001 are good ranges to start with.\n",
    "\n",
    "I would suggest leaving your bias regularizer alone — regularizing the bias typically has very little impact on reducing overfitting.\n",
    "\n",
    "I also suggest leaving the activity_regularizer  at its default value (i.e., no activity regularization).\n",
    "\n",
    "While weight regularization methods operate on weights themselves, f(W), where f is the activation function and W are the weights, an activity regularizer instead operates on the outputs, f(O), where O is the outputs of a layer.\n",
    "\n",
    "Unless there is a very specific reason you’re looking to regularize the output it’s best to leave this parameter alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel_constraint and bias_constraint\n",
    "\n",
    "The final two parameters to the Keras Conv2D class are the kernel_constraint  and bias_constraint .\n",
    "\n",
    "These parameters allow you to impose constraints on the Conv2D layer, including non-negativity, unit normalization, and min-max normalization.\n",
    "\n",
    "You can see the full list of supported constraints in the Keras documentation.\n",
    "\n",
    "Again, I would recommend leaving both the kernel constraint and bias constraint alone unless you have a specific reason to impose constraints on the Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Photo Classification Dataset\n",
    "CIFAR is an acronym that stands for the Canadian Institute For Advanced Research and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset by researchers at the CIFAR institute.\n",
    "\n",
    "The dataset is comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. The class labels and their standard associated integer values are listed below.\n",
    "\n",
    "0. airplane\n",
    "1. automobile\n",
    "2. bird\n",
    "3. cat\n",
    "4. deer\n",
    "5. dog\n",
    "6. frog\n",
    "7. horse\n",
    "8. ship\n",
    "9. truck\n",
    "\n",
    "These are very small images, much smaller than a typical photograph, and the dataset was intended for computer vision research.\n",
    "\n",
    "CIFAR-10 is a well-understood dataset and widely used for benchmarking computer vision algorithms in the field of machine learning. The problem is “solved.” It is relatively straightforward to achieve 80% classification accuracy. Top performance on the problem is achieved by deep learning convolutional neural networks with a classification accuracy above 90% on the test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import plug-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the cifar10 dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from keras import optimizers \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # this line is for hiding the futerwarning of tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a good practice suggests, we need to declare our variables:\n",
    "\n",
    "- *batch_size* – the number of training examples in one forward/ backwards pass. The higher the batch size, the more memory space you’ll need\n",
    "- *num_classes* – number of cifar-10 dataset classes\n",
    "- *one epoch* – one forward pass and one backward pass of all the training examples\n",
    "- *class_names* – an array includes all 10 class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "batch_size = 32 # 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
    "num_classes = 10 # number of outputs possible\n",
    "epochs =  50 # repeat \n",
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print figure with 10 random images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print figure with 10 random images from each\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "for i in range(num_classes):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    idx = np.where(y_train[:]==i)[0]\n",
    "    features_idx = x_train[idx,::]\n",
    "    img_num = np.random.randint(features_idx.shape[0])\n",
    "    im = np.transpose(features_idx[img_num,::],(0,1,2))\n",
    "    ax.set_title(class_names[i])\n",
    "    plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized data\n",
    "It’s good practice to work with normalized data.\n",
    "\n",
    "Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
    "\n",
    "Note, the data is loaded as integers, so we must cast it to float point values in order to perform the division.\n",
    "\n",
    "**Exercise :** \n",
    "1. Cast x_train & x_test in ``float32``.\n",
    "2. Normalize the data so that they have a value in 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code here (4 lines)\n",
    "\n",
    "\n",
    "\n",
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Transform y data into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code here (2 lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s start by defining a simple CNN model. *(4 layers)*\n",
    "\n",
    "We will use a model with four convolutional layers followed by max pooling and a flattening out of the network to fully connected layers to make predictions:\n",
    "\n",
    "1. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "2. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "3. Max Pool layer with size 2×2\n",
    "4. Dropout set to 25%\n",
    "5. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "6. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "7. Max Pool layer with size 2×2\n",
    "8. Dropout set to 25%\n",
    "9. Flatten layer\n",
    "10. Fully connected layer with 512 units and a rectifier activation function\n",
    "11. Dropout set to 50%\n",
    "12. Fully connected output layer with 10 units and a softmax activation function\n",
    "\n",
    "A logarithmic loss function is used with the stochastic gradient descent (SGD) optimization algorithm configured with a large momentum and weight decay start with a learning rate of 0.1.\n",
    "\n",
    "Then we can fit this model with 50 epochs and a batch size of 32.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4_layers():\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], activation='relu')) # Layer 1\n",
    "    model.add(Conv2D(32,(3, 3), activation='relu' )) # Layer 2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu')) # Layer 3\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu')) # Layer 4\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu' ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_4 = model_4_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "![https://www.theux.be/fr/ma-commune/services-communaux/attention.jpg/@@images/image.jpeg](https://www.theux.be/fr/ma-commune/services-communaux/attention.jpg/@@images/image.jpeg)\n",
    "Be careful, training the model with convolution networks is resource-intensive.  \n",
    "If you have a good graphics card it is then better to configure your python environment to use your gpu. \n",
    "\n",
    "In my case, a \"gpu\" environment was already preconfigured with Anaconda. If this is not your case, you will have to do a small google search.\n",
    "\n",
    "The time saving is not negligible, by doing tests, I could see that the training was done 5 times faster using the gpu. (21 seconds per epoch instead of 110 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's go !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4 = cnn_4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores4l = cnn_4.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores4l[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My scrore**  \n",
    "Accuracy: 79.73%  \n",
    "Epochs : 50  \n",
    "Btach_size : 32  \n",
    "Time : 17 min  \n",
    "GPU : GTX 1050  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_4.save(\"cnn4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn4.history['loss'], label='Train Loss')\n",
    "plt.plot(cnn4.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnn4.history['acc'], label='Train accuracy')\n",
    "plt.plot(cnn4.history['val_acc'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is overfiting from the 15th epochs.   \n",
    "So there is no need to continue beyond that with this model and this dataset and the risk of over-interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second variant for 6 Layer model\n",
    "\n",
    "1. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "2. Dropout set to 20%\n",
    "3. Convolutional input layer, 32 feature maps with a size of 3×3, a rectifier activation function\n",
    "4. Max Pool layer with size 2×2\n",
    "5. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "6. Dropout set to 20%\n",
    "7. Convolutional input layer, 64 feature maps with a size of 3×3, a rectifier activation function\n",
    "8. Max Pool layer with size 2×2\n",
    "9. Convolutional input layer, 128 feature maps with a size of 3×3, a rectifier activation function\n",
    "10. Dropout set to 20%\n",
    "11. Convolutional input layer, 128 feature maps with a size of 3×3, a rectifier activation function\n",
    "12. Max Pool layer with size 2×2\n",
    "13. Flatten layer\n",
    "14. Dropout set to 20%\n",
    "15. Fully connected layer with 1024 units and a rectifier activation function and a weight constraint of max norm set to 3\n",
    "16. Dropout set to 20%\n",
    "17. Fully connected output layer with 10 units and a softmax activation function\n",
    "\n",
    "**Exercise :** Create the 6-layer model following the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6_layers():\n",
    "    \n",
    "    model = Sequential()\n",
    "    ### Enter your code here (+- 17 lines)\n",
    "    #### layer 1 : (2 lines)\n",
    "   \n",
    "\n",
    "    #### layer 2 : (2 lines)\n",
    "   \n",
    "\n",
    "    #### layer 3 : (2 lines)\n",
    "\n",
    "    \n",
    "    #### layer 4 : (2 lines)\n",
    "  \n",
    "\n",
    "    #### layer 5 : (2 lines)\n",
    "  \n",
    "\n",
    "    #### layer 6 : (2 lines)\n",
    " \n",
    "\n",
    "    \n",
    "    #### Output (+- 5 lines)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    ### End of your code \n",
    "    \n",
    "    # Compile model \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_6 = model_6_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice :** Display the summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter ypur code here (1 line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should do something like this:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_39 (Conv2D)           (None, 32, 32, 32)        896       \n",
    "_________________________________________________________________\n",
    "dropout_30 (Dropout)         (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_40 (Conv2D)           (None, 32, 32, 32)        9248      \n",
    "_________________________________________________________________\n",
    "max_pooling2d_20 (MaxPooling (None, 16, 16, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_41 (Conv2D)           (None, 16, 16, 64)        18496     \n",
    "_________________________________________________________________\n",
    "dropout_31 (Dropout)         (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_42 (Conv2D)           (None, 16, 16, 64)        36928     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_21 (MaxPooling (None, 8, 8, 64)          0         \n",
    "_________________________________________________________________\n",
    "conv2d_43 (Conv2D)           (None, 8, 8, 128)         73856     \n",
    "_________________________________________________________________\n",
    "dropout_32 (Dropout)         (None, 8, 8, 128)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_44 (Conv2D)           (None, 8, 8, 128)         147584    \n",
    "_________________________________________________________________\n",
    "max_pooling2d_22 (MaxPooling (None, 4, 4, 128)         0         \n",
    "_________________________________________________________________\n",
    "flatten_9 (Flatten)          (None, 2048)              0         \n",
    "_________________________________________________________________\n",
    "dropout_33 (Dropout)         (None, 2048)              0         \n",
    "_________________________________________________________________\n",
    "dense_15 (Dense)             (None, 1024)              2098176   \n",
    "_________________________________________________________________\n",
    "dropout_34 (Dropout)         (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "dense_16 (Dense)             (None, 10)                10250     \n",
    "=================================================================\n",
    "Total params: 2,395,434\n",
    "Trainable params: 2,395,434\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Fit the model with 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code here (1 line) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model \n",
    "\n",
    "**Exercise :** Save your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code here (1 line) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display histogram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Display histogram with loss and accuracy data in ``cnn6``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display LOSS \n",
    "### Enter your code here (+- 4 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Display ACCURACY\n",
    "### Enter your code here (+- 4 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores6l = cnn_6.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores6l[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My score :**  \n",
    "Accuracy: 77.89%  \n",
    "Epochs : 15  \n",
    "Btach_size : 32  \n",
    "Time : 8 min  \n",
    "GPU : GTX 1050   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the dataset \n",
    "Last but not least, we have to augment the pictures with the help of a Keras native library ImageDataGenerator. Essentially, you manipulate the pictures in order to cover more ground. It rotates, flipping and shifting the pictures and generates “distorted” images from the initial dataset. You can find more information about this technique here. Finally, we have to compile the model and train the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_data_augmentation(model, data_augmentation = True, epochs = epochs, batch_s = batch_size):\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "            width_shift_range=0.1,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.5,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None,\n",
    "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "            validation_split=0.0)\n",
    "\n",
    "        # Compute quantities required for feature-wise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                         batch_size=batch_s),\n",
    "                            steps_per_epoch=len(x_train) / batch_s,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(x_test, y_test),\n",
    "                            workers=4)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data_augmentation(cnn_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits overfiting with Early Stopping\n",
    "\n",
    "Keras supports the early stopping of training via a callback called EarlyStopping.\n",
    "\n",
    "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
    "\n",
    "The EarlyStopping callback is configured when instantiated via arguments.\n",
    "\n",
    "The “monitor” allows you to specify the performance measure to monitor in order to end training. Recall from the previous section that the calculation of measures on the validation dataset will have the ‘val_‘ prefix, such as ‘val_loss‘ for the loss on the validation dataset.\n",
    "\n",
    "````python \n",
    "es = EarlyStopping(monitor='val_loss')\n",
    "````\n",
    "\n",
    "Based on the choice of performance measure, the “mode” argument will need to be specified as whether the objective of the chosen metric is to increase (maximize or ‘max‘) or to decrease (minimize or ‘min‘).\n",
    "\n",
    "For example, we would seek a minimum for validation loss and a minimum for validation mean squared error, whereas we would seek a maximum for validation accuracy. \n",
    "\n",
    "````python \n",
    "es = EarlyStopping(monitor='val_loss', mode='min')\n",
    "````\n",
    "By default, mode is set to ‘auto‘ and knows that you want to minimize loss or maximize accuracy.\n",
    "\n",
    "That is all that is needed for the simplest form of early stopping. Training will stop when the chosen performance measure stops improving. To discover the training epoch on which training was stopped, the “verbose” argument can be set to 1. Once stopped, the callback will print the epoch number.\n",
    "\n",
    "````python\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "````\n",
    "Often, the first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better.\n",
    "\n",
    "We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument.\n",
    "\n",
    "````python\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "````\n",
    "The exact amount of patience will vary between models and problems. Reviewing plots of your performance measure can be very useful to get an idea of how noisy the optimization process for your model on your data may be.\n",
    "\n",
    "By default, any change in the performance measure, no matter how fractional, will be considered an improvement. You may want to consider an improvement that is a specific increment, such as 1 unit for mean squared error or 1% for accuracy. This can be specified via the “min_delta” argument.\n",
    "\n",
    "````python\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', baseline=0.4)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then update the call to the fit() function and specify a list of callbacks via the **“callback”** argument.\n",
    "````python\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[es])\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "cnn_4 = model_4_layers()\n",
    "cnn_4.summary()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min',  verbose=1, patience=5)\n",
    "history = cnn_4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=4000, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go!  \n",
    "As soon as keras notices that the val_loss no longer decreases 5 times in a row, he will stop the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your best model ! \n",
    "\n",
    "There is one last thing we need to do. Save the best model. Indeed, the last trained model is not necessarily the best.  To do this we will use the ``ModelCheckpoint()`` method which will allow us to save only the best trained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=4000, verbose=1, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
